# Generative Pseudo Labelling (GPL)

Adapt your dense Retriever to a new domain without human annotation. 

Dense retrieval methods require a large amount of training data to work well and are sensitive to domain shifts. Generative Pseudo Labelling (GPL) is a technique that can make dense retrievers more effective in a new domain, without needing human annotators. It uses an ensemble of trained models to generate labeled data that are then used to perform domain adaptation on the dense retriever.

If you want to find out more, here's the original [GPL paper](https://arxiv.org/pdf/2112.07577.pdf).

## How It Works in General

Here's what happens if you want to use GPL to adapt a dense retriever to a new domain:
1. A pretrained query-generation model generates questions for your documents. The original model used for that was the T5 encoder-decoder model, which is similar to the Haystack question generator. 
2. A retrieval system uses the generated queries to retrieve negative passages. Negative passages are documents that are not relevant to the query. 
3. The dense retriever is trained using the genrated queries, the source document, and the negative passages. It is trained to give high relevancy scores to the source document but low relevancy scores to a negative passage. This should improve the effectiveness of the dense retriever in the new domain.

## How It Works in Haystack

We added a new node called `PseudoLabelGenerator` and enhanced the `train()` method for EmbeddingRetriever so that you can take advantage of GPL in Haystack. Here are the steps for performing domain adaptation with GPL in Haystack:
1. Initialize `PseudoLabelGenerator` with a `QuestionGenerator` and a Retriever. In thi example, we're going to input Documents from the Document Store, that's why we're also importing and defining a Document Store.
```python
from haystack.nodes import PseudoLabelGenerator
from haystack.document_stores import OpenSearchDocumentStore
from haystack.nodes import BM25Retriever
from haystack.nodes import QuestionGenerator

document_store = OpenSearchDocumentStore()
retriever = BM25Retriever(document_store)
qg = QuestionGenerator(model_name_or_path="doc2query/msmarco-t5-base-v1")
plg = PseudoLabelGenerator(qg, retriever)
```
2. Provide your documents in the `run()` method of `PseudoLabelGenerator`. These need to be Haystack Documents so ensure that you preprocess your files.
```python
train_examples = []
for idx, doc in enumerate(document_store):
    output_samples = plg.run(documents=[doc])
    for item in output_samples:
    train_examples.append(item)
```
3. You receive autogenerated labels as output of the `PseudoLabelGenerator` node.
```python
example??
```
4. Use the autogenerated labels in EmbeddingRetriever as input for its `train()` method.
```python
code example here?
```
5. Train your Retriever and that's it! You get a fine-tuned model.
